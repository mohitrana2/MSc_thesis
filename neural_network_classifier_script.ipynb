{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a14a86d8-4fe6-4477-b56c-e8159e8123c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/localuser/Documents/mohit/new_classifier/NN_classifier'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2dfefb7-031c-4ffb-b0a2-d07923b0a058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 16:59:19.824677: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-29 16:59:20.013866: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "# from tensorflow.keras.layers import BatchNormalization\n",
    "# from tensorflow.keras.layers import ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd25068e-a27b-4f24-9e8f-277d72d405bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_data_csv(filepath):\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(data, min_class_instances=100):\n",
    "    if 'Original_ICD-11' in data:\n",
    "        del data['Original_ICD-11']\n",
    "\n",
    "    X = data.iloc[:, 1:-23]\n",
    "    y = data.iloc[:, -23:]\n",
    "\n",
    "    class_labels = y.columns.tolist()\n",
    "    class_mapping = {label: idx for idx, label in enumerate(class_labels)}\n",
    "    y = y.idxmax(axis=1).map(class_mapping).astype(int)\n",
    "\n",
    "    class_counts = y.value_counts()\n",
    "    valid_classes = class_counts[class_counts >= min_class_instances].index\n",
    "    filtered_indices = y.isin(valid_classes)\n",
    "    X = X[filtered_indices]\n",
    "    y = y[filtered_indices]\n",
    "\n",
    "    class_mapping = {old: new for new, old in enumerate(valid_classes)}\n",
    "    y = y.map(class_mapping)\n",
    "\n",
    "    print(f'Number of classes: {len(valid_classes)}')  # Print number of classes\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Filter features by variance\n",
    "def filter_features_by_variance(X, low_quantile=0.10, high_quantile=0.99):\n",
    "    variances = X.var()\n",
    "    features_to_keep = variances[(variances >= variances.quantile(low_quantile)) & (variances <= variances.quantile(high_quantile))].index\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=variances.index, y=variances.values)\n",
    "    plt.axhline(y=variances.quantile(low_quantile), color='r', linestyle='--', label=f'{low_quantile*100}% Threshold')\n",
    "    plt.axhline(y=variances.quantile(high_quantile), color='b', linestyle='--', label=f'{high_quantile*100}% Threshold')\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.title('Variance of Each Feature')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('variance_plot.png')  # Save figure\n",
    "    plt.close()  # Close the figure to release memory\n",
    "\n",
    "    return X[features_to_keep]\n",
    "\n",
    "# Plot data distribution\n",
    "def plot_histogram(data, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(data.values.flatten(), bins=50, kde=True)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Histogram of Data: {title}')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'histogram_{title}.png')  # Save figure\n",
    "    plt.close()  # Close the figure to release memory\n",
    "\n",
    "# Plot PCA\n",
    "def plot_pca(X, y, title):\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis')\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.title(f'PCA Plot: {title}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'pca_plot_{title}.png')  # Save figure\n",
    "    plt.close()  # Close the figure to release memory\n",
    "\n",
    "# Feature selection and balancing\n",
    "def feature_selection_and_balancing(X, Y, n_features_to_select=200, low_quantile=0.10, high_quantile=0.99):\n",
    "    # Remove feature with lower variance\n",
    "    X = filter_features_by_variance(X, low_quantile, high_quantile)\n",
    "\n",
    "    # Feature Selection using RFE\n",
    "    model = RandomForestClassifier()\n",
    "    rfe = RFE(model, n_features_to_select=n_features_to_select)\n",
    "    X_selected = rfe.fit_transform(X, Y)\n",
    "    selected_features = X.columns[rfe.get_support()]\n",
    "\n",
    "    # Plot histogram of selected features\n",
    "    plot_histogram(pd.DataFrame(X_selected, columns=selected_features), 'Selected Features')\n",
    "\n",
    "    # Plot PCA of selected features\n",
    "    plot_pca(pd.DataFrame(X_selected, columns=selected_features), Y, 'Selected Features')\n",
    "\n",
    "    # Handle imbalanced data using SMOTE\n",
    "    smote = SMOTE()\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_selected, Y)\n",
    "\n",
    "    # Plot histogram of resampled data\n",
    "    plot_histogram(pd.DataFrame(X_resampled, columns=selected_features), 'Resampled Features')\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Split data into training and testing sets\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc30497d-0424-4f05-beab-625a3e437aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateLogger(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.learning_rates = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = K.eval(self.model.optimizer.lr)\n",
    "        self.learning_rates.append(lr)\n",
    "        \n",
    "def neural_network_classifier_with_lr_logging(X_train, y_train, X_test, y_test, layers=[64, 32, 16], dropout_rate=0.2, learning_rate=0.001, epochs=200, batch_size=32):\n",
    "    # One-hot encode the categorical labels\n",
    "    encoder = OneHotEncoder(categories='auto', sparse=False)\n",
    "    y_train_onehot = encoder.fit_transform(np.array(y_train).reshape(-1, 1))\n",
    "    y_test_onehot = encoder.transform(np.array(y_test).reshape(-1, 1))\n",
    "\n",
    "    # Build a simple neural network model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layers[0], activation='tanh', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    for layer_size in layers[1:]:\n",
    "        model.add(Dense(layer_size, activation='tanh'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "    model.add(Dense(len(np.unique(y_train)), activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Create an instance of the LearningRateLogger callback\n",
    "    lr_logger = LearningRateLogger()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train_onehot, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test_onehot), callbacks=[lr_logger])\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test_onehot)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('model_accuracy.png')  # Save figure\n",
    "    plt.close()  # Close the figure to release memory\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(len(np.unique(y_train))):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_onehot[:, i], y_pred_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Plot ROC curve for each class\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    colors = plt.cm.get_cmap('tab10', len(np.unique(y_train)))  # Generate enough colors\n",
    "    for i in range(len(np.unique(y_train))):\n",
    "        plt.plot(fpr[i], tpr[i], color=colors(i), lw=2,\n",
    "                 label=f'ROC curve (class {i}) (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('roc_curve.png')  # Save figure\n",
    "    plt.close()  # Close the figure to release memory\n",
    "\n",
    "    # Print and save classification report\n",
    "    y_pred_classes = np.argmax(y_pred_prob, axis=1)\n",
    "    class_report = classification_report(y_test, y_pred_classes, output_dict=True)\n",
    "    print(classification_report(y_test, y_pred_classes))\n",
    "    class_report_df = pd.DataFrame(class_report).transpose()\n",
    "    class_report_df.to_csv('classification_report.csv', index=True)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_classes)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')  # Save figure\n",
    "    plt.close()  # Close the figure to release memory\n",
    "\n",
    "    return model, lr_logger\n",
    "\n",
    "# Plot learning rate\n",
    "def plot_learning_rate(lr_logger):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(lr_logger.learning_rates, label='Learning Rate')\n",
    "    plt.title('Learning Rate over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.legend()\n",
    "    plt.savefig('learning_rate.png')  # Save figure\n",
    "    plt.show()\n",
    "\n",
    "# # Neural Network Classifier\n",
    "# def neural_network_classifier(X_train, y_train, X_test, y_test, layers=[64, 32, 16], dropout_rate=0.2, learning_rate=0.001, epochs=200, batch_size=32):\n",
    "#     # One-hot encode the categorical labels\n",
    "#     encoder = OneHotEncoder(categories='auto', sparse=False)\n",
    "#     y_train_onehot = encoder.fit_transform(np.array(y_train).reshape(-1, 1))\n",
    "#     y_test_onehot = encoder.transform(np.array(y_test).reshape(-1, 1))\n",
    "\n",
    "#     # Build a simple neural network model\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(layers[0], activation='relu', input_shape=(X_train.shape[1],)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "    \n",
    "#     for layer_size in layers[1:]:\n",
    "#         model.add(Dense(layer_size, activation='relu'))\n",
    "#         model.add(Dropout(dropout_rate))\n",
    "        \n",
    "#     model.add(Dense(len(np.unique(y_train)), activation='softmax'))\n",
    "\n",
    "#     # Compile the model\n",
    "#     model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "#                   loss='categorical_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "#     # Train the model\n",
    "#     history = model.fit(X_train, y_train_onehot, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test_onehot))\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     loss, accuracy = model.evaluate(X_test, y_test_onehot)\n",
    "#     print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "#     # Plot training history\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "#     plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "#     plt.title('Model Accuracy')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.legend()\n",
    "#     plt.savefig('model_accuracy.png')  # Save figure\n",
    "#     plt.close()  # Close the figure to release memory\n",
    "\n",
    "#     # Predict probabilities\n",
    "#     y_pred_prob = model.predict(X_test)\n",
    "\n",
    "#     # Compute ROC curve and ROC area for each class\n",
    "#     fpr = dict()\n",
    "#     tpr = dict()\n",
    "#     roc_auc = dict()\n",
    "#     for i in range(len(np.unique(y_train))):\n",
    "#         fpr[i], tpr[i], _ = roc_curve(y_test_onehot[:, i], y_pred_prob[:, i])\n",
    "#         roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "#     # Plot ROC curve for each class\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     colors = plt.cm.get_cmap('tab10', len(np.unique(y_train)))  # Generate enough colors\n",
    "#     for i in range(len(np.unique(y_train))):\n",
    "#         plt.plot(fpr[i], tpr[i], color=colors(i), lw=2,\n",
    "#                  label=f'ROC curve (class {i}) (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "#     plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "#     plt.ylim([0.0, 1.05])\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "#     plt.savefig('roc_curve.png')  # Save figure\n",
    "#     plt.close()  # Close the figure to release memory\n",
    "\n",
    "#     # Print classification report and confusion matrix\n",
    "#     y_pred_classes = np.argmax(y_pred_prob, axis=1)\n",
    "#     print(classification_report(y_test, y_pred_classes))\n",
    "#     cm = confusion_matrix(y_test, y_pred_classes)\n",
    "#     plt.figure(figsize=(10, 7))\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "#     plt.xlabel('Predicted')\n",
    "#     plt.ylabel('True')\n",
    "#     plt.title('Confusion Matrix')\n",
    "#     plt.savefig('confusion_matrix.png')  # Save figure\n",
    "#     plt.close()  # Close the figure to release memory\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "168fa435-3733-4731-9f28-d2c2c5bc6c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AML001_CD34_6H:BRD-K43389675:10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AML001_PC3_6H:BRD-A45664787:10</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASG001_MCF7_24H:BRD-A13084692-001-05-8:0.08</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASG001_MCF7_24H:BRD-A84481105-003-15-6:0.08</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASG001_MCF7_24H:BRD-K41260949-001-06-7:0.08</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASG001_MCF7_24H:BRD-K71879491-001-17-6:0.08</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRAF001_A375_24H:BRD-K92049597-001-14-1:10</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPC001_HA1E_24H:BRD-K78692225-001-11-2:10</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPC001_HCC515_6H:BRD-K09963420-066-03-4:10</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPC001_PC3_6H:BRD-K48970916-001-03-0:10</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPC002_HA1E_24H:BRD-K39188321-001-03-9:10</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPC002_HCC515_24H:BRD-K47639036-003-03-3:10</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              0\n",
       "rid                                            \n",
       "AML001_CD34_6H:BRD-K43389675:10               0\n",
       "AML001_PC3_6H:BRD-A45664787:10                2\n",
       "ASG001_MCF7_24H:BRD-A13084692-001-05-8:0.08   4\n",
       "ASG001_MCF7_24H:BRD-A84481105-003-15-6:0.08   1\n",
       "ASG001_MCF7_24H:BRD-K41260949-001-06-7:0.08   5\n",
       "ASG001_MCF7_24H:BRD-K71879491-001-17-6:0.08   8\n",
       "BRAF001_A375_24H:BRD-K92049597-001-14-1:10    6\n",
       "CPC001_HA1E_24H:BRD-K78692225-001-11-2:10    11\n",
       "CPC001_HCC515_6H:BRD-K09963420-066-03-4:10    3\n",
       "CPC001_PC3_6H:BRD-K48970916-001-03-0:10       9\n",
       "CPC002_HA1E_24H:BRD-K39188321-001-03-9:10     7\n",
       "CPC002_HCC515_24H:BRD-K47639036-003-03-3:10  10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Main function\n",
    "data = load_data_csv('/home/localuser/Documents/mohit/new_classifier/simplified_gene_expression_data.csv')\n",
    "data.set_index(\"rid\", inplace=True)\n",
    "X, Y = preprocess_data(data, 100)\n",
    "pd.DataFrame(Y).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edea5dab-f238-4e0d-bb8f-70421346ae0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PSME1</th>\n",
       "      <th>ATF1</th>\n",
       "      <th>RHEB</th>\n",
       "      <th>FOXO3</th>\n",
       "      <th>RHOA</th>\n",
       "      <th>IL1B</th>\n",
       "      <th>ASAH1</th>\n",
       "      <th>RALA</th>\n",
       "      <th>ARHGEF12</th>\n",
       "      <th>SOX2</th>\n",
       "      <th>...</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>J</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AML001_CD34_6H:BRD-K43389675:10</th>\n",
       "      <td>0.004217</td>\n",
       "      <td>-0.018644</td>\n",
       "      <td>0.019958</td>\n",
       "      <td>-0.056607</td>\n",
       "      <td>-0.017211</td>\n",
       "      <td>-0.040410</td>\n",
       "      <td>-0.012175</td>\n",
       "      <td>-0.013127</td>\n",
       "      <td>-0.006405</td>\n",
       "      <td>0.060587</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AML001_PC3_6H:BRD-A45664787:10</th>\n",
       "      <td>-0.000762</td>\n",
       "      <td>-0.000233</td>\n",
       "      <td>0.003371</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>0.009972</td>\n",
       "      <td>0.130947</td>\n",
       "      <td>-0.024684</td>\n",
       "      <td>0.017885</td>\n",
       "      <td>0.023541</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AML001_PC3_6H:BRD-K43389675:0.37037</th>\n",
       "      <td>0.004243</td>\n",
       "      <td>-0.019425</td>\n",
       "      <td>-0.009953</td>\n",
       "      <td>0.009629</td>\n",
       "      <td>0.003882</td>\n",
       "      <td>0.026752</td>\n",
       "      <td>0.007623</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.010465</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AML001_PC3_6H:BRD-K43389675:1.11111</th>\n",
       "      <td>0.012194</td>\n",
       "      <td>-0.011197</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>-0.005428</td>\n",
       "      <td>0.014864</td>\n",
       "      <td>0.036932</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>-0.023778</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AML001_PC3_6H:BRD-K43389675:10</th>\n",
       "      <td>0.021779</td>\n",
       "      <td>0.023403</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>-0.047680</td>\n",
       "      <td>0.024381</td>\n",
       "      <td>0.028317</td>\n",
       "      <td>0.027444</td>\n",
       "      <td>-0.001547</td>\n",
       "      <td>-0.014821</td>\n",
       "      <td>0.017767</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        PSME1      ATF1      RHEB     FOXO3  \\\n",
       "rid                                                                           \n",
       "AML001_CD34_6H:BRD-K43389675:10      0.004217 -0.018644  0.019958 -0.056607   \n",
       "AML001_PC3_6H:BRD-A45664787:10      -0.000762 -0.000233  0.003371  0.004516   \n",
       "AML001_PC3_6H:BRD-K43389675:0.37037  0.004243 -0.019425 -0.009953  0.009629   \n",
       "AML001_PC3_6H:BRD-K43389675:1.11111  0.012194 -0.011197  0.001302 -0.005428   \n",
       "AML001_PC3_6H:BRD-K43389675:10       0.021779  0.023403  0.009063 -0.047680   \n",
       "\n",
       "                                         RHOA      IL1B     ASAH1      RALA  \\\n",
       "rid                                                                           \n",
       "AML001_CD34_6H:BRD-K43389675:10     -0.017211 -0.040410 -0.012175 -0.013127   \n",
       "AML001_PC3_6H:BRD-A45664787:10       0.009972  0.130947 -0.024684  0.017885   \n",
       "AML001_PC3_6H:BRD-K43389675:0.37037  0.003882  0.026752  0.007623 -0.001681   \n",
       "AML001_PC3_6H:BRD-K43389675:1.11111  0.014864  0.036932  0.023810  0.020200   \n",
       "AML001_PC3_6H:BRD-K43389675:10       0.024381  0.028317  0.027444 -0.001547   \n",
       "\n",
       "                                     ARHGEF12      SOX2  ...  E  F  G  H  J  \\\n",
       "rid                                                      ...                  \n",
       "AML001_CD34_6H:BRD-K43389675:10     -0.006405  0.060587  ...  0  0  0  0  0   \n",
       "AML001_PC3_6H:BRD-A45664787:10       0.023541  0.004772  ...  0  0  0  0  0   \n",
       "AML001_PC3_6H:BRD-K43389675:0.37037  0.003181  0.010465  ...  0  0  0  0  0   \n",
       "AML001_PC3_6H:BRD-K43389675:1.11111 -0.023778  0.017790  ...  0  0  0  0  0   \n",
       "AML001_PC3_6H:BRD-K43389675:10      -0.014821  0.017767  ...  0  0  0  0  0   \n",
       "\n",
       "                                     L  M  N  Q  S  \n",
       "rid                                                 \n",
       "AML001_CD34_6H:BRD-K43389675:10      0  0  0  0  0  \n",
       "AML001_PC3_6H:BRD-A45664787:10       0  0  0  0  0  \n",
       "AML001_PC3_6H:BRD-K43389675:0.37037  0  0  0  0  0  \n",
       "AML001_PC3_6H:BRD-K43389675:1.11111  0  0  0  0  0  \n",
       "AML001_PC3_6H:BRD-K43389675:10       0  0  0  0  0  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7fc9412-cf01-48c8-91a2-ea0387679d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4062, 1001)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "89df2a4b-6660-423c-b50a-f8ce46f17481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot initial data distribution\n",
    "plot_histogram(X, 'Initial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "84c07d78-37a1-46bb-a42b-01a0308ca73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9252, 869)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b873ee1-417e-4a87-b9e1-904f862217f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection and balancing\n",
    "X, Y = feature_selection_and_balancing(X, Y, n_features_to_select=869)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feff80ea-20b1-4ccf-957e-7cb6889eb62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = split_data(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5648fcb1-e067-42a2-b5af-e67d46affea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7401, 869)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "378a305b-fa0d-4c63-bb07-5f7056e20acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005070</td>\n",
       "      <td>-0.003869</td>\n",
       "      <td>0.011015</td>\n",
       "      <td>-0.007294</td>\n",
       "      <td>0.012601</td>\n",
       "      <td>0.009024</td>\n",
       "      <td>-0.033187</td>\n",
       "      <td>0.003598</td>\n",
       "      <td>-0.019276</td>\n",
       "      <td>-0.013561</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099564</td>\n",
       "      <td>0.006298</td>\n",
       "      <td>0.016808</td>\n",
       "      <td>-0.006931</td>\n",
       "      <td>-0.003309</td>\n",
       "      <td>-0.021726</td>\n",
       "      <td>-0.020709</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.038099</td>\n",
       "      <td>-0.010418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.007105</td>\n",
       "      <td>0.007689</td>\n",
       "      <td>0.041766</td>\n",
       "      <td>-0.029341</td>\n",
       "      <td>-0.023520</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>-0.038563</td>\n",
       "      <td>-0.008192</td>\n",
       "      <td>-0.005559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043354</td>\n",
       "      <td>0.048153</td>\n",
       "      <td>0.017172</td>\n",
       "      <td>-0.007221</td>\n",
       "      <td>-0.046685</td>\n",
       "      <td>0.012369</td>\n",
       "      <td>-0.022328</td>\n",
       "      <td>0.050602</td>\n",
       "      <td>0.003947</td>\n",
       "      <td>0.010832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.007925</td>\n",
       "      <td>-0.030982</td>\n",
       "      <td>-0.004738</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>-0.010274</td>\n",
       "      <td>-0.018272</td>\n",
       "      <td>-0.003345</td>\n",
       "      <td>0.010868</td>\n",
       "      <td>-0.001955</td>\n",
       "      <td>-0.001683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.009041</td>\n",
       "      <td>0.041221</td>\n",
       "      <td>0.022721</td>\n",
       "      <td>0.012917</td>\n",
       "      <td>-0.007608</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>-0.006559</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.001240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015754</td>\n",
       "      <td>-0.057958</td>\n",
       "      <td>0.006048</td>\n",
       "      <td>-0.006698</td>\n",
       "      <td>-0.027890</td>\n",
       "      <td>0.034121</td>\n",
       "      <td>-0.007554</td>\n",
       "      <td>0.069842</td>\n",
       "      <td>-0.044795</td>\n",
       "      <td>0.006218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.017961</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>-0.011386</td>\n",
       "      <td>0.042236</td>\n",
       "      <td>-0.019910</td>\n",
       "      <td>-0.037742</td>\n",
       "      <td>-0.076476</td>\n",
       "      <td>0.055510</td>\n",
       "      <td>-0.017293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.024543</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>0.046433</td>\n",
       "      <td>0.008080</td>\n",
       "      <td>-0.061369</td>\n",
       "      <td>0.010478</td>\n",
       "      <td>-0.000743</td>\n",
       "      <td>0.010922</td>\n",
       "      <td>-0.041808</td>\n",
       "      <td>-0.025067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008283</td>\n",
       "      <td>-0.012824</td>\n",
       "      <td>-0.048242</td>\n",
       "      <td>-0.030528</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>-0.010697</td>\n",
       "      <td>-0.008246</td>\n",
       "      <td>0.044940</td>\n",
       "      <td>-0.052347</td>\n",
       "      <td>-0.034515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.005070 -0.003869  0.011015 -0.007294  0.012601  0.009024 -0.033187   \n",
       "1 -0.007105  0.007689  0.041766 -0.029341 -0.023520  0.001512  0.000160   \n",
       "2 -0.007925 -0.030982 -0.004738  0.004676 -0.010274 -0.018272 -0.003345   \n",
       "3  0.015754 -0.057958  0.006048 -0.006698 -0.027890  0.034121 -0.007554   \n",
       "4  0.024543  0.002542  0.046433  0.008080 -0.061369  0.010478 -0.000743   \n",
       "\n",
       "        7         8         9    ...       190       191       192       193  \\\n",
       "0  0.003598 -0.019276 -0.013561  ... -0.099564  0.006298  0.016808 -0.006931   \n",
       "1 -0.038563 -0.008192 -0.005559  ...  0.043354  0.048153  0.017172 -0.007221   \n",
       "2  0.010868 -0.001955 -0.001683  ...  0.001798  0.009041  0.041221  0.022721   \n",
       "3  0.069842 -0.044795  0.006218  ...  0.004187  0.017961  0.007247 -0.011386   \n",
       "4  0.010922 -0.041808 -0.025067  ... -0.008283 -0.012824 -0.048242 -0.030528   \n",
       "\n",
       "        194       195       196       197       198       199  \n",
       "0 -0.003309 -0.021726 -0.020709  0.002441  0.038099 -0.010418  \n",
       "1 -0.046685  0.012369 -0.022328  0.050602  0.003947  0.010832  \n",
       "2  0.012917 -0.007608  0.025424 -0.006559  0.001228  0.001240  \n",
       "3  0.042236 -0.019910 -0.037742 -0.076476  0.055510 -0.017293  \n",
       "4  0.013245 -0.010697 -0.008246  0.044940 -0.052347 -0.034515  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ed6ac95-7aae-4e25-8996-f76817a2882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.3872 - accuracy: 0.2371 - val_loss: 2.2280 - val_accuracy: 0.4263\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2.0912 - accuracy: 0.4108 - val_loss: 1.9248 - val_accuracy: 0.4770\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.8221 - accuracy: 0.4656 - val_loss: 1.6834 - val_accuracy: 0.5267\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.6032 - accuracy: 0.5287 - val_loss: 1.5106 - val_accuracy: 0.5651\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.4413 - accuracy: 0.5710 - val_loss: 1.3819 - val_accuracy: 0.6078\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1.3138 - accuracy: 0.6123 - val_loss: 1.2805 - val_accuracy: 0.6326\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.2120 - accuracy: 0.6472 - val_loss: 1.2080 - val_accuracy: 0.6542\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.1335 - accuracy: 0.6671 - val_loss: 1.1500 - val_accuracy: 0.6791\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1.0615 - accuracy: 0.6890 - val_loss: 1.1048 - val_accuracy: 0.6894\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.0048 - accuracy: 0.7029 - val_loss: 1.0708 - val_accuracy: 0.6931\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.9513 - accuracy: 0.7177 - val_loss: 1.0417 - val_accuracy: 0.6975\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.9032 - accuracy: 0.7335 - val_loss: 1.0223 - val_accuracy: 0.7072\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.8699 - accuracy: 0.7411 - val_loss: 1.0013 - val_accuracy: 0.7175\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.8365 - accuracy: 0.7554 - val_loss: 0.9837 - val_accuracy: 0.7250\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.8002 - accuracy: 0.7648 - val_loss: 0.9669 - val_accuracy: 0.7288\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.7807 - accuracy: 0.7718 - val_loss: 0.9578 - val_accuracy: 0.7299\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.7550 - accuracy: 0.7791 - val_loss: 0.9502 - val_accuracy: 0.7288\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.7209 - accuracy: 0.7892 - val_loss: 0.9366 - val_accuracy: 0.7293\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.7092 - accuracy: 0.7942 - val_loss: 0.9282 - val_accuracy: 0.7337\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.6857 - accuracy: 0.7962 - val_loss: 0.9220 - val_accuracy: 0.7358\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.6730 - accuracy: 0.8023 - val_loss: 0.9182 - val_accuracy: 0.7337\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.6499 - accuracy: 0.8116 - val_loss: 0.9143 - val_accuracy: 0.7391\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.6454 - accuracy: 0.8064 - val_loss: 0.8998 - val_accuracy: 0.7450\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.6294 - accuracy: 0.8149 - val_loss: 0.8971 - val_accuracy: 0.7461\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.6161 - accuracy: 0.8215 - val_loss: 0.8962 - val_accuracy: 0.7493\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.5932 - accuracy: 0.8202 - val_loss: 0.8936 - val_accuracy: 0.7553\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.5758 - accuracy: 0.8291 - val_loss: 0.8903 - val_accuracy: 0.7542\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.5741 - accuracy: 0.8307 - val_loss: 0.8947 - val_accuracy: 0.7520\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.5654 - accuracy: 0.8314 - val_loss: 0.8842 - val_accuracy: 0.7585\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.5480 - accuracy: 0.8411 - val_loss: 0.8891 - val_accuracy: 0.7607\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.5522 - accuracy: 0.8362 - val_loss: 0.8891 - val_accuracy: 0.7618\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.5438 - accuracy: 0.8369 - val_loss: 0.8917 - val_accuracy: 0.7623\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.5243 - accuracy: 0.8431 - val_loss: 0.8852 - val_accuracy: 0.7634\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.5255 - accuracy: 0.8438 - val_loss: 0.8856 - val_accuracy: 0.7645\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.5283 - accuracy: 0.8418 - val_loss: 0.8902 - val_accuracy: 0.7672\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.5135 - accuracy: 0.8437 - val_loss: 0.8829 - val_accuracy: 0.7704\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.5056 - accuracy: 0.8520 - val_loss: 0.8829 - val_accuracy: 0.7699\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4886 - accuracy: 0.8493 - val_loss: 0.8806 - val_accuracy: 0.7726\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.4972 - accuracy: 0.8485 - val_loss: 0.8757 - val_accuracy: 0.7715\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4812 - accuracy: 0.8545 - val_loss: 0.8811 - val_accuracy: 0.7769\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.4911 - accuracy: 0.8533 - val_loss: 0.8818 - val_accuracy: 0.7736\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.4714 - accuracy: 0.8557 - val_loss: 0.8810 - val_accuracy: 0.7758\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4661 - accuracy: 0.8560 - val_loss: 0.8816 - val_accuracy: 0.7731\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4572 - accuracy: 0.8630 - val_loss: 0.8877 - val_accuracy: 0.7758\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4584 - accuracy: 0.8607 - val_loss: 0.8810 - val_accuracy: 0.7812\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4460 - accuracy: 0.8649 - val_loss: 0.8817 - val_accuracy: 0.7780\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4515 - accuracy: 0.8607 - val_loss: 0.8819 - val_accuracy: 0.7763\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4484 - accuracy: 0.8608 - val_loss: 0.8814 - val_accuracy: 0.7801\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4395 - accuracy: 0.8634 - val_loss: 0.8853 - val_accuracy: 0.7801\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4342 - accuracy: 0.8697 - val_loss: 0.8801 - val_accuracy: 0.7758\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4368 - accuracy: 0.8661 - val_loss: 0.8873 - val_accuracy: 0.7769\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4298 - accuracy: 0.8642 - val_loss: 0.8825 - val_accuracy: 0.7812\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4296 - accuracy: 0.8706 - val_loss: 0.8771 - val_accuracy: 0.7801\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4165 - accuracy: 0.8700 - val_loss: 0.8713 - val_accuracy: 0.7855\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4182 - accuracy: 0.8714 - val_loss: 0.8790 - val_accuracy: 0.7861\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4166 - accuracy: 0.8683 - val_loss: 0.8763 - val_accuracy: 0.7861\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4162 - accuracy: 0.8695 - val_loss: 0.8772 - val_accuracy: 0.7834\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4138 - accuracy: 0.8669 - val_loss: 0.8814 - val_accuracy: 0.7877\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4150 - accuracy: 0.8708 - val_loss: 0.8743 - val_accuracy: 0.7920\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4012 - accuracy: 0.8770 - val_loss: 0.8854 - val_accuracy: 0.7915\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4075 - accuracy: 0.8712 - val_loss: 0.8854 - val_accuracy: 0.7904\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.4040 - accuracy: 0.8722 - val_loss: 0.8847 - val_accuracy: 0.7904\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3985 - accuracy: 0.8749 - val_loss: 0.8830 - val_accuracy: 0.7936\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4114 - accuracy: 0.8702 - val_loss: 0.8816 - val_accuracy: 0.7942\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3833 - accuracy: 0.8812 - val_loss: 0.8755 - val_accuracy: 0.7974\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3851 - accuracy: 0.8804 - val_loss: 0.8826 - val_accuracy: 0.7925\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3951 - accuracy: 0.8750 - val_loss: 0.8803 - val_accuracy: 0.7958\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3922 - accuracy: 0.8774 - val_loss: 0.8745 - val_accuracy: 0.7979\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.3880 - accuracy: 0.8781 - val_loss: 0.8749 - val_accuracy: 0.7942\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.3863 - accuracy: 0.8723 - val_loss: 0.8771 - val_accuracy: 0.7942\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3846 - accuracy: 0.8788 - val_loss: 0.8761 - val_accuracy: 0.7963\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.3860 - accuracy: 0.8802 - val_loss: 0.8743 - val_accuracy: 0.7974\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3860 - accuracy: 0.8784 - val_loss: 0.8821 - val_accuracy: 0.7974\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.3760 - accuracy: 0.8815 - val_loss: 0.8802 - val_accuracy: 0.7942\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3659 - accuracy: 0.8835 - val_loss: 0.8825 - val_accuracy: 0.7969\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.3801 - accuracy: 0.8806 - val_loss: 0.8725 - val_accuracy: 0.8012\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3670 - accuracy: 0.8823 - val_loss: 0.8844 - val_accuracy: 0.7947\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3623 - accuracy: 0.8826 - val_loss: 0.8835 - val_accuracy: 0.8023\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3622 - accuracy: 0.8833 - val_loss: 0.8836 - val_accuracy: 0.8017\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.3573 - accuracy: 0.8860 - val_loss: 0.8825 - val_accuracy: 0.7990\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3577 - accuracy: 0.8849 - val_loss: 0.8885 - val_accuracy: 0.7990\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3629 - accuracy: 0.8847 - val_loss: 0.8808 - val_accuracy: 0.8028\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.3540 - accuracy: 0.8895 - val_loss: 0.8866 - val_accuracy: 0.7985\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3554 - accuracy: 0.8845 - val_loss: 0.8897 - val_accuracy: 0.8006\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3604 - accuracy: 0.8857 - val_loss: 0.8913 - val_accuracy: 0.8006\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.3533 - accuracy: 0.8842 - val_loss: 0.8968 - val_accuracy: 0.7996\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3533 - accuracy: 0.8853 - val_loss: 0.8998 - val_accuracy: 0.7996\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3501 - accuracy: 0.8869 - val_loss: 0.8965 - val_accuracy: 0.7985\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.3501 - accuracy: 0.8874 - val_loss: 0.8983 - val_accuracy: 0.8001\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.3521 - accuracy: 0.8845 - val_loss: 0.8960 - val_accuracy: 0.7990\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3407 - accuracy: 0.8896 - val_loss: 0.8967 - val_accuracy: 0.7979\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3425 - accuracy: 0.8896 - val_loss: 0.8933 - val_accuracy: 0.8012\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.3467 - accuracy: 0.8858 - val_loss: 0.8886 - val_accuracy: 0.8044\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3524 - accuracy: 0.8843 - val_loss: 0.8939 - val_accuracy: 0.8061\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3241 - accuracy: 0.8901 - val_loss: 0.8987 - val_accuracy: 0.8012\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.3337 - accuracy: 0.8951 - val_loss: 0.8992 - val_accuracy: 0.7958\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.3374 - accuracy: 0.8911 - val_loss: 0.9009 - val_accuracy: 0.8023\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.3506 - accuracy: 0.8866 - val_loss: 0.9047 - val_accuracy: 0.8017\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3347 - accuracy: 0.8897 - val_loss: 0.9017 - val_accuracy: 0.8006\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.3292 - accuracy: 0.8916 - val_loss: 0.9077 - val_accuracy: 0.8039\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3207 - accuracy: 0.8966 - val_loss: 0.9038 - val_accuracy: 0.8012\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3282 - accuracy: 0.8964 - val_loss: 0.9056 - val_accuracy: 0.8055\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.3230 - accuracy: 0.8972 - val_loss: 0.9045 - val_accuracy: 0.8066\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.3255 - accuracy: 0.8956 - val_loss: 0.9108 - val_accuracy: 0.8023\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.3167 - accuracy: 0.8974 - val_loss: 0.8943 - val_accuracy: 0.8055\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.3332 - accuracy: 0.8930 - val_loss: 0.8953 - val_accuracy: 0.8033\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.3264 - accuracy: 0.8976 - val_loss: 0.8999 - val_accuracy: 0.7985\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.3284 - accuracy: 0.8911 - val_loss: 0.9055 - val_accuracy: 0.8017\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3111 - accuracy: 0.8951 - val_loss: 0.9020 - val_accuracy: 0.8028\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3182 - accuracy: 0.8942 - val_loss: 0.9066 - val_accuracy: 0.8050\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.8972 - val_loss: 0.9058 - val_accuracy: 0.8044\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.8985 - val_loss: 0.9062 - val_accuracy: 0.8093\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.3261 - accuracy: 0.8931 - val_loss: 0.9153 - val_accuracy: 0.8044\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3089 - accuracy: 0.8968 - val_loss: 0.9136 - val_accuracy: 0.8023\n",
      "Epoch 115/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.3109 - accuracy: 0.8996 - val_loss: 0.9202 - val_accuracy: 0.8033\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3211 - accuracy: 0.8947 - val_loss: 0.9206 - val_accuracy: 0.8077\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3066 - accuracy: 0.8995 - val_loss: 0.9093 - val_accuracy: 0.8033\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3069 - accuracy: 0.9001 - val_loss: 0.9069 - val_accuracy: 0.8066\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3076 - accuracy: 0.8977 - val_loss: 0.9214 - val_accuracy: 0.7979\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3095 - accuracy: 0.8941 - val_loss: 0.9253 - val_accuracy: 0.8055\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.3011 - accuracy: 0.9018 - val_loss: 0.9247 - val_accuracy: 0.8055\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3115 - accuracy: 0.8995 - val_loss: 0.9165 - val_accuracy: 0.8033\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3126 - accuracy: 0.8972 - val_loss: 0.9160 - val_accuracy: 0.8017\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3037 - accuracy: 0.9012 - val_loss: 0.9198 - val_accuracy: 0.8050\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.3060 - accuracy: 0.8983 - val_loss: 0.9224 - val_accuracy: 0.8033\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.3020 - accuracy: 0.8989 - val_loss: 0.9251 - val_accuracy: 0.8044\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.3046 - accuracy: 0.8992 - val_loss: 0.9248 - val_accuracy: 0.8044\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2935 - accuracy: 0.8999 - val_loss: 0.9289 - val_accuracy: 0.8044\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2993 - accuracy: 0.9023 - val_loss: 0.9260 - val_accuracy: 0.8055\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2932 - accuracy: 0.9011 - val_loss: 0.9218 - val_accuracy: 0.8044\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2974 - accuracy: 0.9051 - val_loss: 0.9324 - val_accuracy: 0.8071\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2958 - accuracy: 0.9023 - val_loss: 0.9313 - val_accuracy: 0.8093\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2964 - accuracy: 0.9007 - val_loss: 0.9334 - val_accuracy: 0.8044\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2898 - accuracy: 0.9030 - val_loss: 0.9264 - val_accuracy: 0.8028\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2892 - accuracy: 0.9037 - val_loss: 0.9285 - val_accuracy: 0.8033\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2879 - accuracy: 0.9034 - val_loss: 0.9251 - val_accuracy: 0.8055\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2953 - accuracy: 0.9041 - val_loss: 0.9266 - val_accuracy: 0.8098\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2878 - accuracy: 0.9047 - val_loss: 0.9304 - val_accuracy: 0.8120\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2766 - accuracy: 0.9081 - val_loss: 0.9428 - val_accuracy: 0.8028\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2875 - accuracy: 0.9049 - val_loss: 0.9403 - val_accuracy: 0.8039\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2879 - accuracy: 0.9074 - val_loss: 0.9455 - val_accuracy: 0.8001\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.2892 - accuracy: 0.9053 - val_loss: 0.9340 - val_accuracy: 0.8012\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2755 - accuracy: 0.9074 - val_loss: 0.9404 - val_accuracy: 0.8061\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2851 - accuracy: 0.9056 - val_loss: 0.9388 - val_accuracy: 0.8077\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2820 - accuracy: 0.9054 - val_loss: 0.9441 - val_accuracy: 0.8066\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2798 - accuracy: 0.9088 - val_loss: 0.9518 - val_accuracy: 0.8071\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2786 - accuracy: 0.9079 - val_loss: 0.9538 - val_accuracy: 0.8071\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2761 - accuracy: 0.9081 - val_loss: 0.9552 - val_accuracy: 0.8066\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2828 - accuracy: 0.9049 - val_loss: 0.9477 - val_accuracy: 0.8050\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2816 - accuracy: 0.9076 - val_loss: 0.9568 - val_accuracy: 0.8066\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2870 - accuracy: 0.9041 - val_loss: 0.9506 - val_accuracy: 0.8055\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2804 - accuracy: 0.9051 - val_loss: 0.9465 - val_accuracy: 0.8033\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2777 - accuracy: 0.9080 - val_loss: 0.9471 - val_accuracy: 0.8104\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2706 - accuracy: 0.9080 - val_loss: 0.9557 - val_accuracy: 0.8082\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2811 - accuracy: 0.9066 - val_loss: 0.9611 - val_accuracy: 0.8050\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2774 - accuracy: 0.9057 - val_loss: 0.9602 - val_accuracy: 0.8023\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2748 - accuracy: 0.9100 - val_loss: 0.9662 - val_accuracy: 0.8082\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2753 - accuracy: 0.9096 - val_loss: 0.9641 - val_accuracy: 0.8028\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2690 - accuracy: 0.9079 - val_loss: 0.9576 - val_accuracy: 0.8039\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2800 - accuracy: 0.9053 - val_loss: 0.9604 - val_accuracy: 0.8071\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2716 - accuracy: 0.9091 - val_loss: 0.9665 - val_accuracy: 0.8071\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2858 - accuracy: 0.9074 - val_loss: 0.9679 - val_accuracy: 0.8044\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2732 - accuracy: 0.9088 - val_loss: 0.9606 - val_accuracy: 0.8088\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2726 - accuracy: 0.9099 - val_loss: 0.9702 - val_accuracy: 0.8050\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2670 - accuracy: 0.9088 - val_loss: 0.9625 - val_accuracy: 0.8098\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2606 - accuracy: 0.9143 - val_loss: 0.9630 - val_accuracy: 0.8061\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2696 - accuracy: 0.9135 - val_loss: 0.9645 - val_accuracy: 0.8055\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2595 - accuracy: 0.9116 - val_loss: 0.9651 - val_accuracy: 0.8082\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2703 - accuracy: 0.9068 - val_loss: 0.9676 - val_accuracy: 0.8050\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2615 - accuracy: 0.9145 - val_loss: 0.9680 - val_accuracy: 0.8044\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.2684 - accuracy: 0.9120 - val_loss: 0.9739 - val_accuracy: 0.8039\n",
      "Epoch 172/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2612 - accuracy: 0.9133 - val_loss: 0.9716 - val_accuracy: 0.8033\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2709 - accuracy: 0.9103 - val_loss: 0.9736 - val_accuracy: 0.8071\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2620 - accuracy: 0.9131 - val_loss: 0.9688 - val_accuracy: 0.8055\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2739 - accuracy: 0.9047 - val_loss: 0.9648 - val_accuracy: 0.8098\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2566 - accuracy: 0.9119 - val_loss: 0.9754 - val_accuracy: 0.8104\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2605 - accuracy: 0.9126 - val_loss: 0.9783 - val_accuracy: 0.8044\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2560 - accuracy: 0.9143 - val_loss: 0.9681 - val_accuracy: 0.8093\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2600 - accuracy: 0.9137 - val_loss: 0.9762 - val_accuracy: 0.8071\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2566 - accuracy: 0.9128 - val_loss: 0.9749 - val_accuracy: 0.8109\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2572 - accuracy: 0.9161 - val_loss: 0.9735 - val_accuracy: 0.8104\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2616 - accuracy: 0.9124 - val_loss: 0.9706 - val_accuracy: 0.8104\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2539 - accuracy: 0.9180 - val_loss: 0.9699 - val_accuracy: 0.8104\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2604 - accuracy: 0.9116 - val_loss: 0.9682 - val_accuracy: 0.8050\n",
      "Epoch 185/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2552 - accuracy: 0.9168 - val_loss: 0.9762 - val_accuracy: 0.8093\n",
      "Epoch 186/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2603 - accuracy: 0.9114 - val_loss: 0.9769 - val_accuracy: 0.8104\n",
      "Epoch 187/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2538 - accuracy: 0.9143 - val_loss: 0.9756 - val_accuracy: 0.8088\n",
      "Epoch 188/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2494 - accuracy: 0.9162 - val_loss: 0.9778 - val_accuracy: 0.8066\n",
      "Epoch 189/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2403 - accuracy: 0.9199 - val_loss: 0.9682 - val_accuracy: 0.8109\n",
      "Epoch 190/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2459 - accuracy: 0.9178 - val_loss: 0.9698 - val_accuracy: 0.8088\n",
      "Epoch 191/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2425 - accuracy: 0.9166 - val_loss: 0.9667 - val_accuracy: 0.8120\n",
      "Epoch 192/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2505 - accuracy: 0.9141 - val_loss: 0.9756 - val_accuracy: 0.8142\n",
      "Epoch 193/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2482 - accuracy: 0.9154 - val_loss: 0.9733 - val_accuracy: 0.8115\n",
      "Epoch 194/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2468 - accuracy: 0.9173 - val_loss: 0.9769 - val_accuracy: 0.8115\n",
      "Epoch 195/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.2573 - accuracy: 0.9138 - val_loss: 0.9775 - val_accuracy: 0.8109\n",
      "Epoch 196/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2416 - accuracy: 0.9174 - val_loss: 0.9772 - val_accuracy: 0.8082\n",
      "Epoch 197/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2490 - accuracy: 0.9146 - val_loss: 0.9832 - val_accuracy: 0.8061\n",
      "Epoch 198/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2397 - accuracy: 0.9210 - val_loss: 0.9809 - val_accuracy: 0.8098\n",
      "Epoch 199/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2505 - accuracy: 0.9172 - val_loss: 0.9869 - val_accuracy: 0.8109\n",
      "Epoch 200/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2446 - accuracy: 0.9185 - val_loss: 0.9887 - val_accuracy: 0.8088\n",
      "Epoch 201/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2430 - accuracy: 0.9138 - val_loss: 0.9940 - val_accuracy: 0.8033\n",
      "Epoch 202/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2474 - accuracy: 0.9137 - val_loss: 0.9806 - val_accuracy: 0.8104\n",
      "Epoch 203/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2462 - accuracy: 0.9154 - val_loss: 0.9885 - val_accuracy: 0.8061\n",
      "Epoch 204/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2361 - accuracy: 0.9193 - val_loss: 0.9843 - val_accuracy: 0.8066\n",
      "Epoch 205/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2440 - accuracy: 0.9211 - val_loss: 0.9759 - val_accuracy: 0.8071\n",
      "Epoch 206/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2404 - accuracy: 0.9197 - val_loss: 0.9748 - val_accuracy: 0.8093\n",
      "Epoch 207/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2321 - accuracy: 0.9238 - val_loss: 0.9655 - val_accuracy: 0.8125\n",
      "Epoch 208/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2329 - accuracy: 0.9208 - val_loss: 0.9682 - val_accuracy: 0.8093\n",
      "Epoch 209/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2397 - accuracy: 0.9174 - val_loss: 0.9812 - val_accuracy: 0.8066\n",
      "Epoch 210/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2389 - accuracy: 0.9184 - val_loss: 0.9844 - val_accuracy: 0.8088\n",
      "Epoch 211/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2401 - accuracy: 0.9184 - val_loss: 0.9843 - val_accuracy: 0.8066\n",
      "Epoch 212/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2423 - accuracy: 0.9191 - val_loss: 0.9896 - val_accuracy: 0.8088\n",
      "Epoch 213/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2430 - accuracy: 0.9172 - val_loss: 0.9856 - val_accuracy: 0.8109\n",
      "Epoch 214/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2520 - accuracy: 0.9150 - val_loss: 0.9837 - val_accuracy: 0.8104\n",
      "Epoch 215/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2462 - accuracy: 0.9177 - val_loss: 0.9868 - val_accuracy: 0.8082\n",
      "Epoch 216/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2283 - accuracy: 0.9207 - val_loss: 0.9992 - val_accuracy: 0.8066\n",
      "Epoch 217/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2293 - accuracy: 0.9215 - val_loss: 1.0018 - val_accuracy: 0.8066\n",
      "Epoch 218/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2206 - accuracy: 0.9250 - val_loss: 1.0014 - val_accuracy: 0.8028\n",
      "Epoch 219/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2310 - accuracy: 0.9242 - val_loss: 0.9971 - val_accuracy: 0.8044\n",
      "Epoch 220/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2300 - accuracy: 0.9201 - val_loss: 0.9931 - val_accuracy: 0.8050\n",
      "Epoch 221/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2326 - accuracy: 0.9191 - val_loss: 1.0018 - val_accuracy: 0.8066\n",
      "Epoch 222/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2309 - accuracy: 0.9226 - val_loss: 0.9980 - val_accuracy: 0.8077\n",
      "Epoch 223/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2324 - accuracy: 0.9219 - val_loss: 1.0130 - val_accuracy: 0.8082\n",
      "Epoch 224/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2316 - accuracy: 0.9201 - val_loss: 1.0168 - val_accuracy: 0.8109\n",
      "Epoch 225/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2326 - accuracy: 0.9220 - val_loss: 1.0121 - val_accuracy: 0.8104\n",
      "Epoch 226/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2287 - accuracy: 0.9234 - val_loss: 1.0142 - val_accuracy: 0.8071\n",
      "Epoch 227/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2449 - accuracy: 0.9169 - val_loss: 1.0055 - val_accuracy: 0.8077\n",
      "Epoch 228/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2377 - accuracy: 0.9173 - val_loss: 0.9993 - val_accuracy: 0.8098\n",
      "Epoch 229/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2307 - accuracy: 0.9223 - val_loss: 1.0074 - val_accuracy: 0.8077\n",
      "Epoch 230/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2351 - accuracy: 0.9187 - val_loss: 1.0028 - val_accuracy: 0.8115\n",
      "Epoch 231/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2263 - accuracy: 0.9222 - val_loss: 0.9989 - val_accuracy: 0.8109\n",
      "Epoch 232/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2262 - accuracy: 0.9214 - val_loss: 1.0080 - val_accuracy: 0.8071\n",
      "Epoch 233/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2213 - accuracy: 0.9261 - val_loss: 1.0141 - val_accuracy: 0.8071\n",
      "Epoch 234/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2314 - accuracy: 0.9230 - val_loss: 1.0258 - val_accuracy: 0.8061\n",
      "Epoch 235/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2269 - accuracy: 0.9200 - val_loss: 1.0176 - val_accuracy: 0.8039\n",
      "Epoch 236/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2294 - accuracy: 0.9206 - val_loss: 1.0156 - val_accuracy: 0.8023\n",
      "Epoch 237/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2206 - accuracy: 0.9228 - val_loss: 1.0185 - val_accuracy: 0.8055\n",
      "Epoch 238/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2133 - accuracy: 0.9284 - val_loss: 1.0192 - val_accuracy: 0.8039\n",
      "Epoch 239/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2252 - accuracy: 0.9260 - val_loss: 1.0295 - val_accuracy: 0.8050\n",
      "Epoch 240/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2230 - accuracy: 0.9230 - val_loss: 1.0210 - val_accuracy: 0.8088\n",
      "Epoch 241/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2299 - accuracy: 0.9214 - val_loss: 1.0200 - val_accuracy: 0.8082\n",
      "Epoch 242/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2241 - accuracy: 0.9251 - val_loss: 1.0175 - val_accuracy: 0.8088\n",
      "Epoch 243/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2174 - accuracy: 0.9245 - val_loss: 1.0169 - val_accuracy: 0.8098\n",
      "Epoch 244/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2132 - accuracy: 0.9278 - val_loss: 1.0130 - val_accuracy: 0.8093\n",
      "Epoch 245/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2207 - accuracy: 0.9253 - val_loss: 1.0234 - val_accuracy: 0.8050\n",
      "Epoch 246/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2252 - accuracy: 0.9261 - val_loss: 1.0162 - val_accuracy: 0.8039\n",
      "Epoch 247/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2244 - accuracy: 0.9228 - val_loss: 1.0155 - val_accuracy: 0.8055\n",
      "Epoch 248/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2250 - accuracy: 0.9208 - val_loss: 1.0226 - val_accuracy: 0.8082\n",
      "Epoch 249/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2224 - accuracy: 0.9235 - val_loss: 1.0331 - val_accuracy: 0.8098\n",
      "Epoch 250/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2226 - accuracy: 0.9256 - val_loss: 1.0302 - val_accuracy: 0.8071\n",
      "Epoch 251/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2165 - accuracy: 0.9249 - val_loss: 1.0369 - val_accuracy: 0.8088\n",
      "Epoch 252/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.2181 - accuracy: 0.9249 - val_loss: 1.0257 - val_accuracy: 0.8077\n",
      "Epoch 253/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2191 - accuracy: 0.9227 - val_loss: 1.0272 - val_accuracy: 0.8104\n",
      "Epoch 254/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2258 - accuracy: 0.9223 - val_loss: 1.0258 - val_accuracy: 0.8115\n",
      "Epoch 255/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2241 - accuracy: 0.9231 - val_loss: 1.0355 - val_accuracy: 0.8088\n",
      "Epoch 256/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2067 - accuracy: 0.9289 - val_loss: 1.0435 - val_accuracy: 0.8082\n",
      "Epoch 257/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2245 - accuracy: 0.9210 - val_loss: 1.0310 - val_accuracy: 0.8066\n",
      "Epoch 258/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2101 - accuracy: 0.9269 - val_loss: 1.0260 - val_accuracy: 0.8082\n",
      "Epoch 259/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2095 - accuracy: 0.9301 - val_loss: 1.0379 - val_accuracy: 0.8077\n",
      "Epoch 260/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2080 - accuracy: 0.9278 - val_loss: 1.0398 - val_accuracy: 0.8115\n",
      "Epoch 261/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2099 - accuracy: 0.9266 - val_loss: 1.0400 - val_accuracy: 0.8093\n",
      "Epoch 262/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2154 - accuracy: 0.9258 - val_loss: 1.0306 - val_accuracy: 0.8109\n",
      "Epoch 263/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2055 - accuracy: 0.9264 - val_loss: 1.0294 - val_accuracy: 0.8071\n",
      "Epoch 264/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2110 - accuracy: 0.9276 - val_loss: 1.0266 - val_accuracy: 0.8093\n",
      "Epoch 265/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2080 - accuracy: 0.9261 - val_loss: 1.0355 - val_accuracy: 0.8082\n",
      "Epoch 266/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2092 - accuracy: 0.9289 - val_loss: 1.0381 - val_accuracy: 0.8066\n",
      "Epoch 267/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2101 - accuracy: 0.9274 - val_loss: 1.0380 - val_accuracy: 0.8120\n",
      "Epoch 268/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2004 - accuracy: 0.9303 - val_loss: 1.0415 - val_accuracy: 0.8055\n",
      "Epoch 269/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2188 - accuracy: 0.9249 - val_loss: 1.0413 - val_accuracy: 0.8077\n",
      "Epoch 270/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2024 - accuracy: 0.9305 - val_loss: 1.0393 - val_accuracy: 0.8055\n",
      "Epoch 271/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2058 - accuracy: 0.9281 - val_loss: 1.0455 - val_accuracy: 0.8088\n",
      "Epoch 272/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2042 - accuracy: 0.9333 - val_loss: 1.0442 - val_accuracy: 0.8136\n",
      "Epoch 273/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2046 - accuracy: 0.9312 - val_loss: 1.0507 - val_accuracy: 0.8066\n",
      "Epoch 274/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2013 - accuracy: 0.9327 - val_loss: 1.0421 - val_accuracy: 0.8066\n",
      "Epoch 275/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2043 - accuracy: 0.9300 - val_loss: 1.0507 - val_accuracy: 0.8093\n",
      "Epoch 276/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2118 - accuracy: 0.9276 - val_loss: 1.0476 - val_accuracy: 0.8088\n",
      "Epoch 277/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1982 - accuracy: 0.9338 - val_loss: 1.0469 - val_accuracy: 0.8071\n",
      "Epoch 278/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2122 - accuracy: 0.9277 - val_loss: 1.0440 - val_accuracy: 0.8104\n",
      "Epoch 279/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1958 - accuracy: 0.9296 - val_loss: 1.0544 - val_accuracy: 0.8109\n",
      "Epoch 280/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1978 - accuracy: 0.9333 - val_loss: 1.0483 - val_accuracy: 0.8082\n",
      "Epoch 281/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1957 - accuracy: 0.9324 - val_loss: 1.0614 - val_accuracy: 0.8115\n",
      "Epoch 282/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2020 - accuracy: 0.9299 - val_loss: 1.0508 - val_accuracy: 0.8098\n",
      "Epoch 283/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2067 - accuracy: 0.9292 - val_loss: 1.0484 - val_accuracy: 0.8088\n",
      "Epoch 284/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1921 - accuracy: 0.9339 - val_loss: 1.0437 - val_accuracy: 0.8104\n",
      "Epoch 285/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1894 - accuracy: 0.9357 - val_loss: 1.0625 - val_accuracy: 0.8071\n",
      "Epoch 286/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1931 - accuracy: 0.9323 - val_loss: 1.0620 - val_accuracy: 0.8093\n",
      "Epoch 287/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2070 - accuracy: 0.9278 - val_loss: 1.0612 - val_accuracy: 0.8088\n",
      "Epoch 288/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2013 - accuracy: 0.9299 - val_loss: 1.0658 - val_accuracy: 0.8082\n",
      "Epoch 289/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1878 - accuracy: 0.9369 - val_loss: 1.0600 - val_accuracy: 0.8098\n",
      "Epoch 290/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1959 - accuracy: 0.9322 - val_loss: 1.0563 - val_accuracy: 0.8125\n",
      "Epoch 291/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1937 - accuracy: 0.9342 - val_loss: 1.0510 - val_accuracy: 0.8163\n",
      "Epoch 292/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2002 - accuracy: 0.9285 - val_loss: 1.0612 - val_accuracy: 0.8120\n",
      "Epoch 293/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1941 - accuracy: 0.9350 - val_loss: 1.0567 - val_accuracy: 0.8077\n",
      "Epoch 294/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2014 - accuracy: 0.9292 - val_loss: 1.0618 - val_accuracy: 0.8066\n",
      "Epoch 295/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2025 - accuracy: 0.9323 - val_loss: 1.0499 - val_accuracy: 0.8055\n",
      "Epoch 296/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1921 - accuracy: 0.9326 - val_loss: 1.0511 - val_accuracy: 0.8055\n",
      "Epoch 297/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.1912 - accuracy: 0.9345 - val_loss: 1.0504 - val_accuracy: 0.8104\n",
      "Epoch 298/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1983 - accuracy: 0.9296 - val_loss: 1.0528 - val_accuracy: 0.8066\n",
      "Epoch 299/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1880 - accuracy: 0.9381 - val_loss: 1.0551 - val_accuracy: 0.8039\n",
      "Epoch 300/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1896 - accuracy: 0.9364 - val_loss: 1.0578 - val_accuracy: 0.8066\n",
      "Epoch 301/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1934 - accuracy: 0.9320 - val_loss: 1.0530 - val_accuracy: 0.8077\n",
      "Epoch 302/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1882 - accuracy: 0.9373 - val_loss: 1.0565 - val_accuracy: 0.8109\n",
      "Epoch 303/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1810 - accuracy: 0.9387 - val_loss: 1.0565 - val_accuracy: 0.8125\n",
      "Epoch 304/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1909 - accuracy: 0.9366 - val_loss: 1.0587 - val_accuracy: 0.8109\n",
      "Epoch 305/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1880 - accuracy: 0.9373 - val_loss: 1.0565 - val_accuracy: 0.8098\n",
      "Epoch 306/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1851 - accuracy: 0.9376 - val_loss: 1.0601 - val_accuracy: 0.8104\n",
      "Epoch 307/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1875 - accuracy: 0.9351 - val_loss: 1.0639 - val_accuracy: 0.8088\n",
      "Epoch 308/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1918 - accuracy: 0.9326 - val_loss: 1.0691 - val_accuracy: 0.8088\n",
      "Epoch 309/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1865 - accuracy: 0.9380 - val_loss: 1.0805 - val_accuracy: 0.8055\n",
      "Epoch 310/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1816 - accuracy: 0.9401 - val_loss: 1.0778 - val_accuracy: 0.8071\n",
      "Epoch 311/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1870 - accuracy: 0.9354 - val_loss: 1.0831 - val_accuracy: 0.8066\n",
      "Epoch 312/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1835 - accuracy: 0.9354 - val_loss: 1.0840 - val_accuracy: 0.8088\n",
      "Epoch 313/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1882 - accuracy: 0.9333 - val_loss: 1.0890 - val_accuracy: 0.8071\n",
      "Epoch 314/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1983 - accuracy: 0.9320 - val_loss: 1.0821 - val_accuracy: 0.8077\n",
      "Epoch 315/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1900 - accuracy: 0.9331 - val_loss: 1.0810 - val_accuracy: 0.8066\n",
      "Epoch 316/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1834 - accuracy: 0.9377 - val_loss: 1.0828 - val_accuracy: 0.8077\n",
      "Epoch 317/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1949 - accuracy: 0.9318 - val_loss: 1.0830 - val_accuracy: 0.8077\n",
      "Epoch 318/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1860 - accuracy: 0.9357 - val_loss: 1.0792 - val_accuracy: 0.8077\n",
      "Epoch 319/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1840 - accuracy: 0.9372 - val_loss: 1.0768 - val_accuracy: 0.8066\n",
      "Epoch 320/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1924 - accuracy: 0.9345 - val_loss: 1.0833 - val_accuracy: 0.8039\n",
      "Epoch 321/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1905 - accuracy: 0.9350 - val_loss: 1.0864 - val_accuracy: 0.8050\n",
      "Epoch 322/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1967 - accuracy: 0.9346 - val_loss: 1.0862 - val_accuracy: 0.8088\n",
      "Epoch 323/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1957 - accuracy: 0.9315 - val_loss: 1.0845 - val_accuracy: 0.8044\n",
      "Epoch 324/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1772 - accuracy: 0.9389 - val_loss: 1.0915 - val_accuracy: 0.8066\n",
      "Epoch 325/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1744 - accuracy: 0.9393 - val_loss: 1.0923 - val_accuracy: 0.8082\n",
      "Epoch 326/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1813 - accuracy: 0.9385 - val_loss: 1.0924 - val_accuracy: 0.8077\n",
      "Epoch 327/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1814 - accuracy: 0.9370 - val_loss: 1.0838 - val_accuracy: 0.8061\n",
      "Epoch 328/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1936 - accuracy: 0.9333 - val_loss: 1.0847 - val_accuracy: 0.8088\n",
      "Epoch 329/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1765 - accuracy: 0.9384 - val_loss: 1.0789 - val_accuracy: 0.8104\n",
      "Epoch 330/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1786 - accuracy: 0.9357 - val_loss: 1.0832 - val_accuracy: 0.8104\n",
      "Epoch 331/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1802 - accuracy: 0.9380 - val_loss: 1.0878 - val_accuracy: 0.8066\n",
      "Epoch 332/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1801 - accuracy: 0.9397 - val_loss: 1.0854 - val_accuracy: 0.8082\n",
      "Epoch 333/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1816 - accuracy: 0.9380 - val_loss: 1.0862 - val_accuracy: 0.8125\n",
      "Epoch 334/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1900 - accuracy: 0.9357 - val_loss: 1.0878 - val_accuracy: 0.8109\n",
      "Epoch 335/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1660 - accuracy: 0.9445 - val_loss: 1.0925 - val_accuracy: 0.8077\n",
      "Epoch 336/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1862 - accuracy: 0.9373 - val_loss: 1.0824 - val_accuracy: 0.8066\n",
      "Epoch 337/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1819 - accuracy: 0.9347 - val_loss: 1.0944 - val_accuracy: 0.8082\n",
      "Epoch 338/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1820 - accuracy: 0.9389 - val_loss: 1.1014 - val_accuracy: 0.8071\n",
      "Epoch 339/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1826 - accuracy: 0.9365 - val_loss: 1.0949 - val_accuracy: 0.8104\n",
      "Epoch 340/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1747 - accuracy: 0.9400 - val_loss: 1.1001 - val_accuracy: 0.8082\n",
      "Epoch 341/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1690 - accuracy: 0.9427 - val_loss: 1.0993 - val_accuracy: 0.8088\n",
      "Epoch 342/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1722 - accuracy: 0.9438 - val_loss: 1.1017 - val_accuracy: 0.8071\n",
      "Epoch 343/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1765 - accuracy: 0.9396 - val_loss: 1.0967 - val_accuracy: 0.8077\n",
      "Epoch 344/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1769 - accuracy: 0.9407 - val_loss: 1.1093 - val_accuracy: 0.8050\n",
      "Epoch 345/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1791 - accuracy: 0.9391 - val_loss: 1.1094 - val_accuracy: 0.8050\n",
      "Epoch 346/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1695 - accuracy: 0.9416 - val_loss: 1.0986 - val_accuracy: 0.8077\n",
      "Epoch 347/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1792 - accuracy: 0.9397 - val_loss: 1.0953 - val_accuracy: 0.8082\n",
      "Epoch 348/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1744 - accuracy: 0.9399 - val_loss: 1.1001 - val_accuracy: 0.8066\n",
      "Epoch 349/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1756 - accuracy: 0.9401 - val_loss: 1.1110 - val_accuracy: 0.8082\n",
      "Epoch 350/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1728 - accuracy: 0.9393 - val_loss: 1.1018 - val_accuracy: 0.8093\n",
      "Epoch 351/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1631 - accuracy: 0.9427 - val_loss: 1.0989 - val_accuracy: 0.8055\n",
      "Epoch 352/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1687 - accuracy: 0.9430 - val_loss: 1.1137 - val_accuracy: 0.8039\n",
      "Epoch 353/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1749 - accuracy: 0.9396 - val_loss: 1.1080 - val_accuracy: 0.8066\n",
      "Epoch 354/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1757 - accuracy: 0.9377 - val_loss: 1.1175 - val_accuracy: 0.8071\n",
      "Epoch 355/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1596 - accuracy: 0.9451 - val_loss: 1.1127 - val_accuracy: 0.8055\n",
      "Epoch 356/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1675 - accuracy: 0.9435 - val_loss: 1.1129 - val_accuracy: 0.8066\n",
      "Epoch 357/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1617 - accuracy: 0.9441 - val_loss: 1.1082 - val_accuracy: 0.8066\n",
      "Epoch 358/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1569 - accuracy: 0.9446 - val_loss: 1.1118 - val_accuracy: 0.8050\n",
      "Epoch 359/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1603 - accuracy: 0.9473 - val_loss: 1.1013 - val_accuracy: 0.8104\n",
      "Epoch 360/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1656 - accuracy: 0.9430 - val_loss: 1.1017 - val_accuracy: 0.8071\n",
      "Epoch 361/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1679 - accuracy: 0.9415 - val_loss: 1.1079 - val_accuracy: 0.8077\n",
      "Epoch 362/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1630 - accuracy: 0.9460 - val_loss: 1.1046 - val_accuracy: 0.8082\n",
      "Epoch 363/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1633 - accuracy: 0.9443 - val_loss: 1.1154 - val_accuracy: 0.8088\n",
      "Epoch 364/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1624 - accuracy: 0.9460 - val_loss: 1.1086 - val_accuracy: 0.8115\n",
      "Epoch 365/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1805 - accuracy: 0.9387 - val_loss: 1.1104 - val_accuracy: 0.8093\n",
      "Epoch 366/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1757 - accuracy: 0.9392 - val_loss: 1.1083 - val_accuracy: 0.8082\n",
      "Epoch 367/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1490 - accuracy: 0.9483 - val_loss: 1.1176 - val_accuracy: 0.8093\n",
      "Epoch 368/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1670 - accuracy: 0.9422 - val_loss: 1.1169 - val_accuracy: 0.8077\n",
      "Epoch 369/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1596 - accuracy: 0.9443 - val_loss: 1.1058 - val_accuracy: 0.8077\n",
      "Epoch 370/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1632 - accuracy: 0.9433 - val_loss: 1.1082 - val_accuracy: 0.8050\n",
      "Epoch 371/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1610 - accuracy: 0.9455 - val_loss: 1.1042 - val_accuracy: 0.8050\n",
      "Epoch 372/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1592 - accuracy: 0.9454 - val_loss: 1.0994 - val_accuracy: 0.8082\n",
      "Epoch 373/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1606 - accuracy: 0.9411 - val_loss: 1.1100 - val_accuracy: 0.8082\n",
      "Epoch 374/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1595 - accuracy: 0.9451 - val_loss: 1.0988 - val_accuracy: 0.8093\n",
      "Epoch 375/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1635 - accuracy: 0.9404 - val_loss: 1.1114 - val_accuracy: 0.8039\n",
      "Epoch 376/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1580 - accuracy: 0.9468 - val_loss: 1.1180 - val_accuracy: 0.8071\n",
      "Epoch 377/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1649 - accuracy: 0.9434 - val_loss: 1.1309 - val_accuracy: 0.8109\n",
      "Epoch 378/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1661 - accuracy: 0.9401 - val_loss: 1.1242 - val_accuracy: 0.8098\n",
      "Epoch 379/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1655 - accuracy: 0.9414 - val_loss: 1.1315 - val_accuracy: 0.8071\n",
      "Epoch 380/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1616 - accuracy: 0.9428 - val_loss: 1.1214 - val_accuracy: 0.8077\n",
      "Epoch 381/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1644 - accuracy: 0.9422 - val_loss: 1.1181 - val_accuracy: 0.8088\n",
      "Epoch 382/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1492 - accuracy: 0.9476 - val_loss: 1.1187 - val_accuracy: 0.8077\n",
      "Epoch 383/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1593 - accuracy: 0.9435 - val_loss: 1.1304 - val_accuracy: 0.8050\n",
      "Epoch 384/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1644 - accuracy: 0.9430 - val_loss: 1.1323 - val_accuracy: 0.8055\n",
      "Epoch 385/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1627 - accuracy: 0.9438 - val_loss: 1.1260 - val_accuracy: 0.8039\n",
      "Epoch 386/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1535 - accuracy: 0.9503 - val_loss: 1.1351 - val_accuracy: 0.8104\n",
      "Epoch 387/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1671 - accuracy: 0.9401 - val_loss: 1.1236 - val_accuracy: 0.8066\n",
      "Epoch 388/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1608 - accuracy: 0.9441 - val_loss: 1.1253 - val_accuracy: 0.8088\n",
      "Epoch 389/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1583 - accuracy: 0.9458 - val_loss: 1.1198 - val_accuracy: 0.8120\n",
      "Epoch 390/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1553 - accuracy: 0.9458 - val_loss: 1.1151 - val_accuracy: 0.8098\n",
      "Epoch 391/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1695 - accuracy: 0.9400 - val_loss: 1.1162 - val_accuracy: 0.8098\n",
      "Epoch 392/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1492 - accuracy: 0.9477 - val_loss: 1.1200 - val_accuracy: 0.8082\n",
      "Epoch 393/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1610 - accuracy: 0.9446 - val_loss: 1.1246 - val_accuracy: 0.8066\n",
      "Epoch 394/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1539 - accuracy: 0.9464 - val_loss: 1.1352 - val_accuracy: 0.8055\n",
      "Epoch 395/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1592 - accuracy: 0.9458 - val_loss: 1.1225 - val_accuracy: 0.8088\n",
      "Epoch 396/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1527 - accuracy: 0.9472 - val_loss: 1.1265 - val_accuracy: 0.8088\n",
      "Epoch 397/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1474 - accuracy: 0.9507 - val_loss: 1.1224 - val_accuracy: 0.8088\n",
      "Epoch 398/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1501 - accuracy: 0.9489 - val_loss: 1.1332 - val_accuracy: 0.8088\n",
      "Epoch 399/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1495 - accuracy: 0.9484 - val_loss: 1.1227 - val_accuracy: 0.8104\n",
      "Epoch 400/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1531 - accuracy: 0.9464 - val_loss: 1.1329 - val_accuracy: 0.8104\n",
      "Epoch 401/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1581 - accuracy: 0.9464 - val_loss: 1.1216 - val_accuracy: 0.8093\n",
      "Epoch 402/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1502 - accuracy: 0.9491 - val_loss: 1.1258 - val_accuracy: 0.8071\n",
      "Epoch 403/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1464 - accuracy: 0.9522 - val_loss: 1.1131 - val_accuracy: 0.8104\n",
      "Epoch 404/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1545 - accuracy: 0.9446 - val_loss: 1.1237 - val_accuracy: 0.8088\n",
      "Epoch 405/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1548 - accuracy: 0.9451 - val_loss: 1.1207 - val_accuracy: 0.8066\n",
      "Epoch 406/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1512 - accuracy: 0.9461 - val_loss: 1.1237 - val_accuracy: 0.8115\n",
      "Epoch 407/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1544 - accuracy: 0.9453 - val_loss: 1.1270 - val_accuracy: 0.8109\n",
      "Epoch 408/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1541 - accuracy: 0.9488 - val_loss: 1.1268 - val_accuracy: 0.8115\n",
      "Epoch 409/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1541 - accuracy: 0.9496 - val_loss: 1.1229 - val_accuracy: 0.8115\n",
      "Epoch 410/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1527 - accuracy: 0.9477 - val_loss: 1.1314 - val_accuracy: 0.8104\n",
      "Epoch 411/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1571 - accuracy: 0.9454 - val_loss: 1.1285 - val_accuracy: 0.8109\n",
      "Epoch 412/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1497 - accuracy: 0.9477 - val_loss: 1.1493 - val_accuracy: 0.8098\n",
      "Epoch 413/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1461 - accuracy: 0.9493 - val_loss: 1.1495 - val_accuracy: 0.8082\n",
      "Epoch 414/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1579 - accuracy: 0.9414 - val_loss: 1.1353 - val_accuracy: 0.8115\n",
      "Epoch 415/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1463 - accuracy: 0.9488 - val_loss: 1.1379 - val_accuracy: 0.8109\n",
      "Epoch 416/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1571 - accuracy: 0.9461 - val_loss: 1.1388 - val_accuracy: 0.8120\n",
      "Epoch 417/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1520 - accuracy: 0.9472 - val_loss: 1.1317 - val_accuracy: 0.8088\n",
      "Epoch 418/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1500 - accuracy: 0.9480 - val_loss: 1.1443 - val_accuracy: 0.8098\n",
      "Epoch 419/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1463 - accuracy: 0.9495 - val_loss: 1.1494 - val_accuracy: 0.8104\n",
      "Epoch 420/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1426 - accuracy: 0.9524 - val_loss: 1.1496 - val_accuracy: 0.8104\n",
      "Epoch 421/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1513 - accuracy: 0.9483 - val_loss: 1.1483 - val_accuracy: 0.8088\n",
      "Epoch 422/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1538 - accuracy: 0.9497 - val_loss: 1.1561 - val_accuracy: 0.8131\n",
      "Epoch 423/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1490 - accuracy: 0.9503 - val_loss: 1.1516 - val_accuracy: 0.8093\n",
      "Epoch 424/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1477 - accuracy: 0.9493 - val_loss: 1.1525 - val_accuracy: 0.8098\n",
      "Epoch 425/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1461 - accuracy: 0.9507 - val_loss: 1.1416 - val_accuracy: 0.8088\n",
      "Epoch 426/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1512 - accuracy: 0.9468 - val_loss: 1.1478 - val_accuracy: 0.8109\n",
      "Epoch 427/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1532 - accuracy: 0.9484 - val_loss: 1.1482 - val_accuracy: 0.8061\n",
      "Epoch 428/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1428 - accuracy: 0.9499 - val_loss: 1.1548 - val_accuracy: 0.8055\n",
      "Epoch 429/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1453 - accuracy: 0.9507 - val_loss: 1.1526 - val_accuracy: 0.8044\n",
      "Epoch 430/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1509 - accuracy: 0.9461 - val_loss: 1.1501 - val_accuracy: 0.8066\n",
      "Epoch 431/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1365 - accuracy: 0.9519 - val_loss: 1.1548 - val_accuracy: 0.8050\n",
      "Epoch 432/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1497 - accuracy: 0.9499 - val_loss: 1.1558 - val_accuracy: 0.8071\n",
      "Epoch 433/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1428 - accuracy: 0.9508 - val_loss: 1.1641 - val_accuracy: 0.8088\n",
      "Epoch 434/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1434 - accuracy: 0.9514 - val_loss: 1.1608 - val_accuracy: 0.8115\n",
      "Epoch 435/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1381 - accuracy: 0.9539 - val_loss: 1.1654 - val_accuracy: 0.8077\n",
      "Epoch 436/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1397 - accuracy: 0.9510 - val_loss: 1.1674 - val_accuracy: 0.8098\n",
      "Epoch 437/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1430 - accuracy: 0.9510 - val_loss: 1.1623 - val_accuracy: 0.8061\n",
      "Epoch 438/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1446 - accuracy: 0.9514 - val_loss: 1.1538 - val_accuracy: 0.8093\n",
      "Epoch 439/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1453 - accuracy: 0.9497 - val_loss: 1.1552 - val_accuracy: 0.8082\n",
      "Epoch 440/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1452 - accuracy: 0.9522 - val_loss: 1.1514 - val_accuracy: 0.8071\n",
      "Epoch 441/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1434 - accuracy: 0.9505 - val_loss: 1.1456 - val_accuracy: 0.8077\n",
      "Epoch 442/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1427 - accuracy: 0.9503 - val_loss: 1.1595 - val_accuracy: 0.8077\n",
      "Epoch 443/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1313 - accuracy: 0.9557 - val_loss: 1.1594 - val_accuracy: 0.8061\n",
      "Epoch 444/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1358 - accuracy: 0.9511 - val_loss: 1.1676 - val_accuracy: 0.8098\n",
      "Epoch 445/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1333 - accuracy: 0.9543 - val_loss: 1.1675 - val_accuracy: 0.8077\n",
      "Epoch 446/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1464 - accuracy: 0.9510 - val_loss: 1.1487 - val_accuracy: 0.8125\n",
      "Epoch 447/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1426 - accuracy: 0.9503 - val_loss: 1.1643 - val_accuracy: 0.8088\n",
      "Epoch 448/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1422 - accuracy: 0.9518 - val_loss: 1.1581 - val_accuracy: 0.8109\n",
      "Epoch 449/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1352 - accuracy: 0.9538 - val_loss: 1.1552 - val_accuracy: 0.8120\n",
      "Epoch 450/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1498 - accuracy: 0.9469 - val_loss: 1.1556 - val_accuracy: 0.8131\n",
      "Epoch 451/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1435 - accuracy: 0.9515 - val_loss: 1.1595 - val_accuracy: 0.8109\n",
      "Epoch 452/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1420 - accuracy: 0.9510 - val_loss: 1.1551 - val_accuracy: 0.8082\n",
      "Epoch 453/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1426 - accuracy: 0.9527 - val_loss: 1.1617 - val_accuracy: 0.8109\n",
      "Epoch 454/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1306 - accuracy: 0.9551 - val_loss: 1.1651 - val_accuracy: 0.8109\n",
      "Epoch 455/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1388 - accuracy: 0.9532 - val_loss: 1.1663 - val_accuracy: 0.8115\n",
      "Epoch 456/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1437 - accuracy: 0.9504 - val_loss: 1.1642 - val_accuracy: 0.8125\n",
      "Epoch 457/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.1434 - accuracy: 0.9500 - val_loss: 1.1599 - val_accuracy: 0.8115\n",
      "Epoch 458/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1370 - accuracy: 0.9537 - val_loss: 1.1640 - val_accuracy: 0.8115\n",
      "Epoch 459/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1371 - accuracy: 0.9541 - val_loss: 1.1533 - val_accuracy: 0.8131\n",
      "Epoch 460/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1386 - accuracy: 0.9526 - val_loss: 1.1410 - val_accuracy: 0.8120\n",
      "Epoch 461/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1291 - accuracy: 0.9572 - val_loss: 1.1600 - val_accuracy: 0.8115\n",
      "Epoch 462/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1262 - accuracy: 0.9572 - val_loss: 1.1498 - val_accuracy: 0.8125\n",
      "Epoch 463/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1342 - accuracy: 0.9542 - val_loss: 1.1519 - val_accuracy: 0.8125\n",
      "Epoch 464/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1379 - accuracy: 0.9535 - val_loss: 1.1654 - val_accuracy: 0.8109\n",
      "Epoch 465/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1367 - accuracy: 0.9516 - val_loss: 1.1581 - val_accuracy: 0.8109\n",
      "Epoch 466/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1387 - accuracy: 0.9526 - val_loss: 1.1548 - val_accuracy: 0.8131\n",
      "Epoch 467/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1431 - accuracy: 0.9507 - val_loss: 1.1545 - val_accuracy: 0.8136\n",
      "Epoch 468/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1344 - accuracy: 0.9527 - val_loss: 1.1557 - val_accuracy: 0.8131\n",
      "Epoch 469/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1355 - accuracy: 0.9527 - val_loss: 1.1557 - val_accuracy: 0.8093\n",
      "Epoch 470/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1358 - accuracy: 0.9524 - val_loss: 1.1660 - val_accuracy: 0.8104\n",
      "Epoch 471/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1353 - accuracy: 0.9553 - val_loss: 1.1650 - val_accuracy: 0.8093\n",
      "Epoch 472/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1250 - accuracy: 0.9561 - val_loss: 1.1655 - val_accuracy: 0.8055\n",
      "Epoch 473/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1349 - accuracy: 0.9545 - val_loss: 1.1559 - val_accuracy: 0.8088\n",
      "Epoch 474/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1480 - accuracy: 0.9480 - val_loss: 1.1615 - val_accuracy: 0.8115\n",
      "Epoch 475/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1193 - accuracy: 0.9604 - val_loss: 1.1512 - val_accuracy: 0.8093\n",
      "Epoch 476/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1299 - accuracy: 0.9582 - val_loss: 1.1447 - val_accuracy: 0.8142\n",
      "Epoch 477/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1215 - accuracy: 0.9582 - val_loss: 1.1607 - val_accuracy: 0.8120\n",
      "Epoch 478/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1328 - accuracy: 0.9534 - val_loss: 1.1568 - val_accuracy: 0.8093\n",
      "Epoch 479/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1361 - accuracy: 0.9555 - val_loss: 1.1529 - val_accuracy: 0.8109\n",
      "Epoch 480/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1327 - accuracy: 0.9532 - val_loss: 1.1641 - val_accuracy: 0.8082\n",
      "Epoch 481/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1292 - accuracy: 0.9566 - val_loss: 1.1613 - val_accuracy: 0.8088\n",
      "Epoch 482/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1339 - accuracy: 0.9551 - val_loss: 1.1585 - val_accuracy: 0.8093\n",
      "Epoch 483/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1240 - accuracy: 0.9569 - val_loss: 1.1434 - val_accuracy: 0.8142\n",
      "Epoch 484/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1365 - accuracy: 0.9523 - val_loss: 1.1594 - val_accuracy: 0.8115\n",
      "Epoch 485/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1245 - accuracy: 0.9582 - val_loss: 1.1691 - val_accuracy: 0.8098\n",
      "Epoch 486/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1314 - accuracy: 0.9542 - val_loss: 1.1749 - val_accuracy: 0.8131\n",
      "Epoch 487/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1337 - accuracy: 0.9534 - val_loss: 1.1722 - val_accuracy: 0.8115\n",
      "Epoch 488/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1292 - accuracy: 0.9561 - val_loss: 1.1802 - val_accuracy: 0.8115\n",
      "Epoch 489/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1377 - accuracy: 0.9538 - val_loss: 1.1651 - val_accuracy: 0.8147\n",
      "Epoch 490/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1237 - accuracy: 0.9597 - val_loss: 1.1661 - val_accuracy: 0.8147\n",
      "Epoch 491/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1290 - accuracy: 0.9572 - val_loss: 1.1731 - val_accuracy: 0.8098\n",
      "Epoch 492/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1295 - accuracy: 0.9576 - val_loss: 1.1722 - val_accuracy: 0.8082\n",
      "Epoch 493/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1193 - accuracy: 0.9576 - val_loss: 1.1783 - val_accuracy: 0.8088\n",
      "Epoch 494/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1353 - accuracy: 0.9528 - val_loss: 1.1781 - val_accuracy: 0.8044\n",
      "Epoch 495/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1243 - accuracy: 0.9582 - val_loss: 1.1741 - val_accuracy: 0.8115\n",
      "Epoch 496/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1201 - accuracy: 0.9561 - val_loss: 1.1699 - val_accuracy: 0.8115\n",
      "Epoch 497/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1230 - accuracy: 0.9566 - val_loss: 1.1679 - val_accuracy: 0.8098\n",
      "Epoch 498/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1238 - accuracy: 0.9568 - val_loss: 1.1754 - val_accuracy: 0.8093\n",
      "Epoch 499/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1314 - accuracy: 0.9522 - val_loss: 1.1899 - val_accuracy: 0.8088\n",
      "Epoch 500/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1297 - accuracy: 0.9538 - val_loss: 1.1818 - val_accuracy: 0.8088\n",
      "58/58 [==============================] - 0s 655us/step - loss: 1.1818 - accuracy: 0.8088\n",
      "Test Accuracy: 80.88%\n",
      "58/58 [==============================] - 0s 756us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.45      0.52       149\n",
      "           1       0.64      0.47      0.54       157\n",
      "           2       0.64      0.51      0.57       174\n",
      "           3       0.70      0.78      0.74       152\n",
      "           4       0.82      0.87      0.85       166\n",
      "           5       0.81      0.86      0.83       155\n",
      "           6       0.75      0.92      0.83       149\n",
      "           7       0.85      0.94      0.89       158\n",
      "           8       0.93      0.99      0.96       152\n",
      "           9       0.92      0.98      0.95       143\n",
      "          10       0.97      0.99      0.98       156\n",
      "          11       0.95      1.00      0.97       140\n",
      "\n",
      "    accuracy                           0.81      1851\n",
      "   macro avg       0.80      0.81      0.80      1851\n",
      "weighted avg       0.80      0.81      0.80      1851\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neural Network Classifier with Learning Rate Logging\n",
    "model, lr_logger = neural_network_classifier_with_lr_logging(X_train, y_train, X_test, y_test, layers = [256, 64, 32], dropout_rate=0.3, learning_rate=0.001, epochs=500, batch_size=512)\n",
    "\n",
    "# # Neural Network Classifier\n",
    "# neural_network_classifier(X_train, y_train, X_test, y_test, layers=[256, 64, 32], dropout_rate=0.3, learning_rate=0.001, epochs=500, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "917aa8a7-6629-4f1a-8ab4-b62599f82f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning rate\n",
    "plot_learning_rate(lr_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9825f502-3b6c-47f8-bd3b-445f610377d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "def build_and_plot_model(layers, dropout_rate, input_shape, output_shape, learning_rate):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Dense(layers[0], activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for layer_size in layers[1:]:\n",
    "        model.add(Dense(layer_size, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(output_shape, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Plot the model\n",
    "    plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "    print(\"Model architecture saved as 'model_architecture.png'\")\n",
    "\n",
    "# Define parameters\n",
    "layers = [256, 64, 32]        # Example: [256, 64, 32]\n",
    "dropout_rate = 0.3            # Example: 0.3\n",
    "input_shape = 200             # Example: number of input features\n",
    "output_shape = 10             # Example: number of classes\n",
    "\n",
    "# Build and plot the model\n",
    "build_and_plot_model(layers, dropout_rate, input_shape, output_shape, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08f2edf0-41e3-4b84-9213-380734ca8a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/localuser/Documents/mohit/new_classifier/NN_classifier'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb091e6-17e6-488d-baac-ce4ce2ed74f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
